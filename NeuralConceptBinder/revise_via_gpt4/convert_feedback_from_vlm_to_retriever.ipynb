{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1168a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import pickle \n",
    "import time\n",
    "from collections import Counter\n",
    "from natsort import natsorted\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utils_bnr import set_seed\n",
    "\n",
    "class Args:\n",
    "    seed = 0\n",
    "    batch_size = 1\n",
    "    num_workers = 4\n",
    "    image_size = 128\n",
    "    image_channels = 3\n",
    "\n",
    "    num_slots = 4\n",
    "    num_blocks = 8\n",
    "    \n",
    "    #-----------------------------------------------------------------#\n",
    "    #!!!!!!!!!!!!!!!!Change this as necessary!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #-----------------------------------------------------------------#\n",
    "    base_path = '../logs/clevr_easy_500_epochs/sysbind_orig_seed2'\n",
    "    rand_imgs_path = '../logs/clevr_easy_500_epochs/random_imgs'\n",
    "    \n",
    "#     base_path = '../logs/clevr4_600_epochs/clevr4_sysbind_orig_seed2'\n",
    "#     rand_imgs_path = '../logs/clevr4_600_epochs/random_imgs'\n",
    "    #-----------------------------------------------------------------#\n",
    "\n",
    "args=Args()\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(args.seed)\n",
    "\n",
    "# set seed\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "revise_dir = f'{args.base_path}/revision'\n",
    "if not os.path.exists(revise_dir):\n",
    "    os.makedirs(revise_dir)\n",
    "args.save_path = revise_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac215e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_response_json(response, split_str='Final Answer: ', verbose=0):\n",
    "    \"\"\"\n",
    "    response: response from the VLM model.\n",
    "    split_str: string that specifies where to cut the VLM response in order to extract the final task answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        answer = response.json()['choices'][0]['message']['content']\n",
    "    except:\n",
    "        print(f'{response.json()} \\n Error')\n",
    "    \n",
    "    if verbose:\n",
    "        print(answer)\n",
    "    \n",
    "    answer_y_n = answer.split(split_str)[-1]\n",
    "    if 'No' not in answer_y_n and 'Yes' not in answer_y_n:\n",
    "        print(answer)        \n",
    "        raise Exception(\"This response isn't binary\") \n",
    "    else:\n",
    "        if 'No' in answer_y_n:\n",
    "            return 0\n",
    "        elif 'Yes' in answer_y_n:\n",
    "            return 1\n",
    "     \n",
    "    \n",
    "def extract_properties_from_response(response):\n",
    "     return '{'+response.json()['choices'][0]['message']['content'].split('{\\n')[-1].split('\\n}')[0]+'}'\n",
    "    \n",
    "        \n",
    "def find_majority(responses, split_str='Final Answer: ' , verbose=0):\n",
    "    \"\"\"\n",
    "    responses: list of response json from VLM model. This list should be the list of multiply querying the model \n",
    "    with the same sample and prompt.\n",
    "    \n",
    "    returns: the majority vote from these responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    votes = [extract_answer_from_response_json(response, split_str, verbose) for response in responses]\n",
    "    \n",
    "    vote_count = Counter(votes)\n",
    "    top_two = vote_count.most_common(2)\n",
    "    if len(top_two)>1 and top_two[0][1] == top_two[1][1]:\n",
    "        # It is a tie\n",
    "        return 0\n",
    "    return top_two[0][0]\n",
    "\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    \n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return client.chat.completions.create(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6ada5",
   "metadata": {},
   "source": [
    "## 1. Query VLM for property lists - N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d95a5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{args.save_path}/responses_knowledge_properties.pkl', 'wb') as f:\n",
    "#     pickle.dump(knowledge_responses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da08cb",
   "metadata": {},
   "source": [
    "## 2. Query VLM for occurence of sub-property per object in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75112739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{args.save_path}/responses_description.pkl', 'wb') as f:\n",
    "#     pickle.dump(responses_description, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544e7df",
   "metadata": {},
   "source": [
    "## 3. Analyse if concept should be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load property list responses\n",
    "# with open(f'{args.save_path}/responses_knowledge_properties.pkl', 'rb') as f:\n",
    "#     knowledge_responses = pickle.load(f)\n",
    "# # load image descriptions based on these property lists\n",
    "# with open(f'{args.save_path}/responses_description.pkl', 'rb') as f:\n",
    "#     responses_description = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_obj_list_from_response(response):\n",
    "#     # get object dict as str from response\n",
    "#     obj_dict_str = response.json()['choices'][0]['message']['content'].split('{')[-1].split('}')[0]\n",
    "#     # extract only property description lists\n",
    "#     obj_list_str = re.split(r'Object\\d+: ', obj_dict_str)[1:]\n",
    "#     obj_list_str = [re.split(r'], ', s)[0]+']' for s in obj_list_str] \n",
    "# #     obj_list_str = [s.split('[')[1].split(']')[0].split(', ') for s in obj_list_str]\n",
    "#     return obj_list_str\n",
    "\n",
    "\n",
    "# def extract_property_lists(response):\n",
    "#     # given response get only the property list relevant string\n",
    "#     property_str = extract_properties_from_response(response)\n",
    "#     # now clean it such that we have two arrays of strings, with each string representing a sub-property\n",
    "#     property_str = property_str.split('{')[-1].split(',}')[0]\n",
    "#     property_str = re.split(r': ', property_str)[1:]\n",
    "#     property_str = [s.split('[')[-1].split(']')[0] for s in property_str]\n",
    "#     property_str = [s.split(', ') for s in property_str]    \n",
    "#     return property_str\n",
    "\n",
    "\n",
    "# def get_bool_prop_list_for_response(response, knowledge_response, verbose=0):\n",
    "    \n",
    "#     # get the object list contained within it\n",
    "#     obj_list_str =  extract_obj_list_from_response(response)\n",
    "#     if verbose:\n",
    "#         print(obj_list_str)\n",
    "\n",
    "#     # get the property list that the description prompt was based on\n",
    "#     property_list_str = extract_property_lists(knowledge_response)\n",
    "\n",
    "#     # iterate over each property and subproperty and check if it occurs in the object description of each object\n",
    "#     # properties_obj_bool represents a dictionary where each property id key contains a boolean array as value\n",
    "#     properties_obj_bool = {}\n",
    "#     for property_id in range(len(property_list_str)):\n",
    "#         # booolean array, for each object we store if it depicts a property\n",
    "#         property_obj_bool = np.zeros((len(obj_list_str), len(property_list_str[property_id])))\n",
    "\n",
    "#         for sub_property_id in range(len(property_list_str[property_id])):\n",
    "\n",
    "#             for obj_id in range(len(obj_list_str)):\n",
    "\n",
    "#                 property_obj_bool[obj_id, sub_property_id] = \\\n",
    "#                     property_list_str[property_id][sub_property_id] in obj_list_str[obj_id]\n",
    "\n",
    "#         properties_obj_bool[property_id] = property_obj_bool\n",
    "    \n",
    "#     return properties_obj_bool\n",
    "\n",
    "\n",
    "# def check_for_deletion(prop_obj_bool_dict):\n",
    "#     delete_bool = []\n",
    "#     for prop_id in prop_obj_bool_dict.keys():\n",
    "#         # count the number of occurances of each subproperty over the objects\n",
    "#         occur_subprop = np.sum(prop_obj_bool_dict[prop_id], axis=0)\n",
    "#         # check if each object has one sub-property, i.e. the occurance of a subproperty equals the number of objs\n",
    "#         delete_bool.append(prop_obj_bool_dict[prop_id].shape[0] in occur_subprop)\n",
    "#     # if at least one subproperty is present across all objects return true\n",
    "#     if np.sum(delete_bool) > 0:\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "   \n",
    "\n",
    "# N_QUERIES = 4\n",
    "# # a little messed up here, but for each block and each concept we now collect the deletion booleans across samples\n",
    "# delete_block_dict = {}\n",
    "# for block_id in responses_description[0].keys():\n",
    "#     delete_concept_dict = {}\n",
    "#     for concept_id in responses_description[0][block_id].keys():\n",
    "#         delete_samples = []\n",
    "# #         for sample_id in responses_description.keys():\n",
    "#         for sample_id in range(N_QUERIES):\n",
    "#             # get description resonse\n",
    "#             response = responses_description[sample_id][block_id][concept_id]\n",
    "#             # get a boolean representation of the object description list, where per object we check if a subproperty is in \n",
    "#             # the corresponding obj list\n",
    "#             # prop_obj_bool_dict[0]: N_Obj x N_SubProps\n",
    "#             prop_obj_bool_dict = get_bool_prop_list_for_response(response, knowledge_responses[sample_id])\n",
    "#             # check if any subprop occurs across all objects\n",
    "#             delete_bool = check_for_deletion(prop_obj_bool_dict)\n",
    "\n",
    "#             delete_samples.append(delete_bool)\n",
    "\n",
    "#         delete_concept_dict[concept_id] = delete_samples\n",
    "\n",
    "#     delete_block_dict[block_id] = delete_concept_dict\n",
    "    \n",
    "# delete_concepts_dict = {}\n",
    "# # now we perform majority vote over the collected deletion booleans\n",
    "# for block_id in delete_block_dict.keys():\n",
    "#     delete_concepts = []\n",
    "#     for concept_id in delete_block_dict[block_id].keys():\n",
    "# #         assert len(delete_block_dict[block_id][concept_id]) % 2 == 1\n",
    "#         vote_count = Counter(delete_block_dict[block_id][concept_id])\n",
    "#         top_two = vote_count.most_common(2)\n",
    "#         if len(top_two)>1 and top_two[0][1] == top_two[1][1]:\n",
    "#             # It is a tie, in this case we choose benefit of the doubt, i.e., don't delete\n",
    "#             pass\n",
    "#         # if majarity vote is true we delete the concept\n",
    "#         elif top_two[0][0]:\n",
    "#             delete_concepts.append(concept_id)\n",
    "#     delete_concepts_dict[block_id] = delete_concepts\n",
    "\n",
    "# delete_concepts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{args.save_path}/concepts_delete_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(delete_concepts_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828543a",
   "metadata": {},
   "source": [
    "### 3.1 Now integrate this feedback into the retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "194fcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{args.base_path}/block_concept_dicts.pkl', 'rb') as f:\n",
    "    ret_corpus = pickle.load(f)\n",
    "with open(f'{args.save_path}/concepts_delete_dict.pkl', 'rb') as f:\n",
    "    delete_concepts_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "777983f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0, 1, 2, 3]\n",
      "Block 0: case 1\n",
      "1\n",
      "[3]\n",
      "Block 1: case 3\n",
      "2\n",
      "[]\n",
      "3\n",
      "[]\n",
      "4\n",
      "[0, 1]\n",
      "Block 4: case 1\n",
      "5\n",
      "[0, 1, 2]\n",
      "Block 5: case 1\n",
      "6\n",
      "[4, 5]\n",
      "Block 6: case 3\n",
      "7\n",
      "[7]\n",
      "Block 7: case 3\n"
     ]
    }
   ],
   "source": [
    "def set_id_from_id_list(id_list, delete_id, set_id=-1):\n",
    "    \"\"\"\n",
    "    This function takes a list of ids, a delete_id which should be deleted and a set_id, i.e., the value which the\n",
    "    deleted ids will be set to instead.\n",
    "    \"\"\"\n",
    "    # identify the ids of the concept-to-be-deleted\n",
    "    rel_ids = id_list == delete_id\n",
    "    # set these to -1\n",
    "    id_list[rel_ids] = set_id\n",
    "    return None\n",
    "\n",
    "\n",
    "def set_id_over_all_representations(block_corpus, delete_id, set_id=-1):\n",
    "    \"\"\"\n",
    "    Iterates over all representations and resets the ids of the cluster id identified as 'delete_id'. \n",
    "    'set_id' is the novel id which the cluster encodings are set to.\n",
    "    \"\"\"\n",
    "    set_id_from_id_list(block_corpus['prototypes']['ids'], \n",
    "                        delete_id=delete_id, \n",
    "                        set_id=set_id)\n",
    "    set_id_from_id_list(block_corpus['exemplars']['ids'], \n",
    "                        delete_id=delete_id, \n",
    "                        set_id=set_id)\n",
    "    set_id_from_id_list(block_corpus['sivm_basis']['ids'], \n",
    "                        delete_id=delete_id, \n",
    "                        set_id=set_id)\n",
    "\n",
    "\n",
    "# def remove_concept(block_corpus, delete_id):\n",
    "#     \"\"\"\n",
    "#     Removes encodings and corresponding information entirely from the cluster identified via 'delete_id'\n",
    "#     \"\"\"\n",
    "#     # i.e. 'prototypes', 'exemplars', 'sivm_basis'\n",
    "#     representation_keys = list(block_corpus.keys())\n",
    "#     representation_keys.remove('params_clustering')\n",
    "\n",
    "#     for representation_key in representation_keys:\n",
    "#         # e.g. 'exemplars', 'exemplar_ids'\n",
    "#         data_keys = list(block_corpus[representation_key].keys())\n",
    "#         # we handle the ids separately\n",
    "#         data_keys.remove('ids')\n",
    "#         # identify which encodings to keep and which not to keep\n",
    "#         del_ids = np.where(block_corpus[representation_key]['ids'] == delete_id)[0]\n",
    "#         keep_ids = np.where(block_corpus[representation_key]['ids'] != delete_id)[0]\n",
    "#         # number of individual clusters altogether\n",
    "#         n_clusters = len(np.unique(block_corpus[representation_key]['ids']))\n",
    "\n",
    "#         for data_key in data_keys:\n",
    "#             # handle the case where we have a list of arrays (e.g. 'exemplar_ids') vs a long list (e.g. 'exemplars')\n",
    "#             if '_ids' in data_key:\n",
    "#                 #len(block_corpus[representation_key][data_key]) == n_clusters:\n",
    "#                 block_corpus[representation_key][data_key] = [\n",
    "#                     ele for idx, ele in enumerate(block_corpus[representation_key][data_key]) if idx != delete_id\n",
    "#                 ]\n",
    "#             else:\n",
    "#                 block_corpus[representation_key][data_key] = [\n",
    "#                     ele for idx, ele in enumerate(block_corpus[representation_key][data_key]) if idx not in del_ids\n",
    "#                 ]\n",
    "#         # finally remove the ids themselves, i.e. keep only relevant ones\n",
    "#         block_corpus[representation_key]['ids'] = block_corpus[representation_key]['ids'][keep_ids]\n",
    "\n",
    "    \n",
    "# update ids in retrieval corpus based on feedback\n",
    "ret_corpus_delete = ret_corpus.copy()\n",
    "for block_id in delete_concepts_dict.keys():\n",
    "\n",
    "    # identify how many clusters exist per block\n",
    "    image_path = f\"{args.base_path}/clustered_exemplars/block{block_id}*.png\"\n",
    "    exemplar_paths = glob.glob(image_path)\n",
    "    exemplar_paths = natsorted(exemplar_paths)\n",
    "    n_clusters_block = len(exemplar_paths)\n",
    "    \n",
    "    print(block_id)\n",
    "    print(delete_concepts_dict[block_id])\n",
    "    \n",
    "    # we now have three cases how to handle deletion\n",
    "    # case 1: all clusters are to be deleted --> we set all cluster ids to 0\n",
    "    # case 2: all clusters, but 1 are to be deleted --> we merge all to-delete clsuters, \n",
    "    #         i.e., set all to-delete cluster ids to one of this set\n",
    "    # case 3: at least two clusters should not be deleted --> we remove the cluster encodings completely of the\n",
    "    #         to-delete clusters\n",
    "    if delete_concepts_dict[block_id]:\n",
    "        # case 1\n",
    "        if len(delete_concepts_dict[block_id]) == n_clusters_block:\n",
    "            print(f'Block {block_id}: case 1')\n",
    "            # set cluster id in corpus to 0 for all representations\n",
    "            for delete_concept_id in delete_concepts_dict[block_id]:\n",
    "                set_id_over_all_representations(\n",
    "                    ret_corpus_delete[block_id], \n",
    "                    delete_id=delete_concept_id, \n",
    "                    set_id=0\n",
    "                )\n",
    "        # case 2\n",
    "        elif len(delete_concepts_dict[block_id]) == (n_clusters_block - 1): \n",
    "            print(f'Block {block_id}: case 2')\n",
    "            # set all to-delete cluster ids to that of first one to delete, essentially merging these\n",
    "            set_id = delete_concepts_dict[block_id][0]\n",
    "            for delete_concept_id in delete_concepts_dict[block_id]:\n",
    "                set_id_over_all_representations(\n",
    "                    ret_corpus_delete[block_id], \n",
    "                    delete_id=delete_concept_id, \n",
    "                    set_id=set_id\n",
    "                )\n",
    "        # case 3\n",
    "        elif len(delete_concepts_dict[block_id]) <= (n_clusters_block - 2):\n",
    "            print(f'Block {block_id}: case 3')\n",
    "            for delete_concept_id in delete_concepts_dict[block_id]:\n",
    "#                 remove_concept(\n",
    "#                     ret_corpus_delete[block_id],\n",
    "#                     delete_id=delete_concept_id,\n",
    "#                 )\n",
    "                set_id_over_all_representations(\n",
    "                    ret_corpus_delete[block_id], \n",
    "                    delete_id=delete_concept_id, \n",
    "                    set_id=-1\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5e7bd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{args.save_path}/block_concept_dicts_revise_delete.pkl', 'wb') as f:\n",
    "    pickle.dump(ret_corpus_delete, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b684c6",
   "metadata": {},
   "source": [
    "## 4. Analyse if concepts should be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e57e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load property list responses\n",
    "with open(f'{args.save_path}/responses_knowledge_properties.pkl', 'rb') as f:\n",
    "    knowledge_responses = pickle.load(f)\n",
    "# load image descriptions based on these property lists\n",
    "with open(f'{args.save_path}/responses_description.pkl', 'rb') as f:\n",
    "    responses_description = pickle.load(f)\n",
    "with open(f'{args.save_path}/concepts_delete_dict.pkl', 'rb') as f:\n",
    "    delete_concepts_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91325ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_properties_bool(prop_obj_bool_dict):\n",
    "    common_properties = {}\n",
    "    for prop_id in prop_obj_bool_dict.keys():\n",
    "#     prop_id = 0\n",
    "        # count the number of occurances of each subproperty over the objects\n",
    "        occur_subprop = np.sum(prop_obj_bool_dict[prop_id], axis=0)\n",
    "        # check if any sub-property occurs for all objs, returns bool array\n",
    "        common_properties[prop_id] = occur_subprop == prop_obj_bool_dict[prop_id].shape[0]\n",
    "    return common_properties\n",
    "\n",
    "\n",
    "N_QUERIES = 7\n",
    "# # a little messed up here, but for each block and each concept we now collect the deletion booleans across samples\n",
    "# delete_block_dict = {}\n",
    "#     delete_concept_dict = {}\n",
    "#     for concept_id in responses_description[0][block_id].keys():\n",
    "#         delete_samples = []\n",
    "# #         for sample_id in responses_description.keys():\n",
    "\n",
    "# identify the common subproperties per concept\n",
    "common_props = {}\n",
    "for sample_id in range(N_QUERIES):\n",
    "    common_props_block = {}\n",
    "    for block_id in responses_description[0].keys():\n",
    "        common_props_concept = {}\n",
    "        for concept_id in responses_description[0][block_id].keys():\n",
    "            if concept_id not in delete_concepts_dict[block_id]:\n",
    "                # get description resonse\n",
    "                response = responses_description[sample_id][block_id][concept_id]\n",
    "                # get a boolean representation of the object description list, where per object we check if a subproperty is in \n",
    "                # the corresponding obj list\n",
    "                # prop_obj_bool_dict[0]: N_Obj x N_SubProps\n",
    "                prop_obj_bool_dict = get_bool_prop_list_for_response(response, knowledge_responses[sample_id])\n",
    "                # # check if any subprop occurs across all objects\n",
    "                # delete_bool = check_for_deletion(prop_obj_bool_dict)\n",
    "                common_props_concept[concept_id] = get_common_properties_bool(prop_obj_bool_dict)\n",
    "        common_props_block[block_id] = common_props_concept\n",
    "    common_props[sample_id] = common_props_block\n",
    "\n",
    "# TODO double check this + see if maybe if at least one false we don\"t consider a merge? --> bronze and yellow get merged via gold\n",
    "# now do pairwaise comparisons between remaining concepts\n",
    "common_props_pairwise = {}\n",
    "for sample_id in range(N_QUERIES):\n",
    "# for sample_id in [1]:\n",
    "    common_props_pairwise_block = {}\n",
    "    for block_id in responses_description[0].keys():\n",
    "#     for block_id in [7]:\n",
    "        common_props_pairwise_concept1 = {}\n",
    "        # get list of concept ids in this block\n",
    "        concept_ids_list = list(common_props[0][block_id].keys())\n",
    "        # and a copy of this\n",
    "        concept_ids_2_list = list(common_props[0][block_id].keys())\n",
    "        # we now perform a lower traingular pairwise comparison\n",
    "        for concept_id1 in concept_ids_list:\n",
    "            # remove the current concept_id1 from the second list\n",
    "            concept_ids_2_list.remove(concept_id1)\n",
    "            common_props_pairwise_concept2 = {}\n",
    "            for concept_id2 in concept_ids_2_list:\n",
    "                # gather the common props of the two concepts\n",
    "                concept1 = common_props[sample_id][block_id][concept_id1]\n",
    "                concept2 = common_props[sample_id][block_id][concept_id2]\n",
    "                # check if they have a pairwise common subprop\n",
    "                common_subprops = []\n",
    "                for prop_id in concept1.keys():\n",
    "                    # test if any concept shares a common subproperty with the other concept\n",
    "                    common_subprops.append(np.sum((concept1[prop_id].astype(int)+concept2[prop_id].astype(int)) > 1))\n",
    "                # test if any concept shares a common subproperty with the other concept\n",
    "                common_props_pairwise_concept2[concept_id2] = int(np.sum(common_subprops) > 0)\n",
    "            common_props_pairwise_concept1[concept_id1] = common_props_pairwise_concept2\n",
    "        common_props_pairwise_block[block_id] = common_props_pairwise_concept1\n",
    "    common_props_pairwise[sample_id] = common_props_pairwise_block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUERIES = 4\n",
    "\n",
    "merge_block_dict_samples = {}\n",
    "for block_id in common_props_pairwise[0].keys():\n",
    "    merge_concept_dict_samples = {}\n",
    "    for concept_id in common_props_pairwise[0][block_id].keys():\n",
    "        merge_concept2_dict_samples = {}\n",
    "        for concept_id2 in common_props_pairwise[0][block_id][concept_id].keys():\n",
    "            tmp = []\n",
    "            for sample_id in range(N_QUERIES):\n",
    "                tmp.append(common_props_pairwise[sample_id][block_id][concept_id][concept_id2])\n",
    "            merge_concept2_dict_samples[concept_id2] = tmp\n",
    "        merge_concept_dict_samples[concept_id] = merge_concept2_dict_samples\n",
    "    merge_block_dict_samples[block_id] = merge_concept_dict_samples\n",
    "    \n",
    "merge_block_dict = {}\n",
    "for block_id in merge_block_dict_samples.keys():\n",
    "    merge_concept_dict = {}\n",
    "    for concept_id in merge_block_dict_samples[block_id].keys():\n",
    "        merge_concept2_dict = {}\n",
    "        for concept_id2 in merge_block_dict_samples[block_id][concept_id].keys():\n",
    "            # check if at least 2 queries produced a merge request\n",
    "            merge_concept2_dict[concept_id2] = 0\n",
    "            if sum(merge_block_dict_samples[block_id][concept_id][concept_id2]) >= 2:\n",
    "                merge_concept2_dict[concept_id2] = 1\n",
    "        merge_concept_dict[concept_id] = merge_concept2_dict\n",
    "    merge_block_dict[block_id] = merge_concept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19266196",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{args.save_path}/concepts_merge_samples_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(merge_block_dict_samples, f)\n",
    "with open(f'{args.save_path}/concepts_merge_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(merge_block_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfeb24",
   "metadata": {},
   "source": [
    "### 4.1 Now integrate this feedback into the retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd377197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{args.save_path}/block_concept_dicts_revise_delete.pkl', 'rb') as f:\n",
    "    ret_corpus_delete = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9293f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_id_with_id2_in_id_list(id_list, to_be_merged_id, merge_to_id):\n",
    "    \"\"\"\n",
    "    This function takes a list of ids, the id that should be merged and one to merge to. \n",
    "    \"\"\"\n",
    "    # identify the ids of the concept-to-be-deleted\n",
    "    rel_ids = id_list == to_be_merged_id\n",
    "    # set these to -1\n",
    "    id_list[rel_ids] = merge_to_id\n",
    "    return None\n",
    "\n",
    "# update ids in retrieval corpus based on feedback\n",
    "# i.e. if the majority of gpt prompts has identified a potential merge\n",
    "# replace the compared-to-concept-id (merge_concept_id2) with the comparing-concept-id (merge_concept_id)\n",
    "ret_corpus_delete_merge = ret_corpus_delete.copy()\n",
    "for block_id in merge_block_dict.keys():\n",
    "    for merge_concept_id in merge_block_dict[block_id].keys():\n",
    "        for merge_concept_id2 in merge_block_dict[block_id][merge_concept_id].keys():\n",
    "            if merge_block_dict[block_id][merge_concept_id][merge_concept_id2]:\n",
    "                merge_id_with_id2_in_id_list(ret_corpus_delete_merge[block_id]['prototypes']['ids'], \n",
    "                                             to_be_merged_id=merge_concept_id2, \n",
    "                                             merge_to_id=merge_concept_id)\n",
    "                merge_id_with_id2_in_id_list(ret_corpus_delete_merge[block_id]['exemplars']['ids'], \n",
    "                                             to_be_merged_id=merge_concept_id2, \n",
    "                                             merge_to_id=merge_concept_id)\n",
    "                merge_id_with_id2_in_id_list(ret_corpus_delete_merge[block_id]['sivm_basis']['ids'], \n",
    "                                             to_be_merged_id=merge_concept_id2, \n",
    "                                             merge_to_id=merge_concept_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290648aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{args.save_path}/block_concept_dicts_revise_delete_merge.pkl', 'wb') as f:\n",
    "    pickle.dump(ret_corpus_delete_merge, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf012528",
   "metadata": {},
   "source": [
    "## (5. Make final query if the concepts should be merged?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

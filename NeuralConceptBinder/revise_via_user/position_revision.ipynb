{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ffec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.image import imread\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from retrievalbinder import NeuralConceptBinder\n",
    "from retrievalbinder import SysBinderImageAutoEncoder\n",
    "from data import CLEVR4_1_WithAnnotations_LeftRight\n",
    "from utils_bnr import set_seed\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--num_workers', type=int, default=0)\n",
    "parser.add_argument('--image_size', type=int, default=128)\n",
    "parser.add_argument('--image_channels', type=int, default=3)\n",
    "parser.add_argument('--data_path', default='data/*.png')\n",
    "parser.add_argument('--perc_imgs', type=float, default=1.0,\n",
    "                    help='The percent of images which the clf model should receive as training images '\n",
    "                         '(between 0 and 1). The test set is always the full one (i.e. 1. is 100%)')\n",
    "parser.add_argument('--log_path', default='../logs/')\n",
    "parser.add_argument('--checkpoint_path', default='../logs/sysbind_orig_seed0/best_model.pt')\n",
    "parser.add_argument('--model_type', choices=['retbind', 'sysbind', 'sysbind_hard', 'sysbind_step'],\n",
    "                    help='Specify whether model type. Either original sysbinder (sysbind) or bind&retrieve (bnr).', default='retbind')\n",
    "parser.add_argument('--use_dp', default=False, action='store_true')\n",
    "parser.add_argument('--name', default=datetime.now().strftime('%Y-%m-%d_%H:%M:%S'),\n",
    "                    help='Name to store the log file as')\n",
    "\n",
    "# arguments for linear probing\n",
    "parser.add_argument('--num_categories', type=int, default=3,\n",
    "                    help='how many categories of attributes')\n",
    "# parser.add_argument('--clf_label_type', default='individual', choices=['combined', 'individual'],\n",
    "#                     help='Specify whether the classification labels should consist of the combined attributes or '\n",
    "#                          'each attribute individually.')\n",
    "parser.add_argument('--clf_type', default=None, choices=['dt', 'rg'],\n",
    "                    help='Specify the linear classifier model. Either decision tree (dt) or ridge regression model '\n",
    "                         '(rg)')\n",
    "\n",
    "# Sysbinder arguments\n",
    "parser.add_argument('--lr_dvae', type=float, default=3e-4)\n",
    "parser.add_argument('--lr_enc', type=float, default=1e-4)\n",
    "parser.add_argument('--lr_dec', type=float, default=3e-4)\n",
    "parser.add_argument('--lr_warmup_steps', type=int, default=30000)\n",
    "parser.add_argument('--lr_half_life', type=int, default=250000)\n",
    "parser.add_argument('--clip', type=float, default=0.05)\n",
    "parser.add_argument('--epochs', type=int, default=500)\n",
    "parser.add_argument('--num_iterations', type=int, default=3)\n",
    "parser.add_argument('--num_slots', type=int, default=4)\n",
    "parser.add_argument('--num_blocks', type=int, default=8)\n",
    "parser.add_argument('--cnn_hidden_size', type=int, default=512)\n",
    "parser.add_argument('--slot_size', type=int, default=2048)\n",
    "parser.add_argument('--mlp_hidden_size', type=int, default=192)\n",
    "parser.add_argument('--num_prototypes', type=int, default=64)\n",
    "parser.add_argument('--temp', type=float, default=1., help='softmax temperature for prototype binding')\n",
    "parser.add_argument('--temp_step', default=False, action='store_true')\n",
    "parser.add_argument('--vocab_size', type=int, default=4096)\n",
    "parser.add_argument('--num_decoder_layers', type=int, default=8)\n",
    "parser.add_argument('--num_decoder_heads', type=int, default=4)\n",
    "parser.add_argument('--d_model', type=int, default=192)\n",
    "parser.add_argument('--dropout', type=float, default=0.1)\n",
    "parser.add_argument('--tau_start', type=float, default=1.0)\n",
    "parser.add_argument('--tau_final', type=float, default=0.1)\n",
    "parser.add_argument('--tau_steps', type=int, default=30000)\n",
    "parser.add_argument('--lr', type=float, default=1e-2, help='Outer learning rate of model')\n",
    "parser.add_argument('--binarize', default=False, action='store_true',\n",
    "                    help='Should the encodings of the sysbinder be binarized?')\n",
    "parser.add_argument('--attention_codes', default=False, action='store_true',\n",
    "                    help='Should the sysbinder prototype attention values be used as encodings?')\n",
    "\n",
    "# R&B arguments\n",
    "parser.add_argument('--retrieval_corpus_path', default='../logs/sysbind_orig_seed0/block_concept_dicts.pkl')\n",
    "parser.add_argument('--retrieval_encs', default='proto-exem',\n",
    "                    choices=['proto', 'exem', 'basis', 'proto-exem', 'proto-exem-basis'])\n",
    "parser.add_argument('--majority_vote', default=True, action='store_true',\n",
    "                    help='If set then the retrieval binder takes the majority vote of the topk nearest encodings to '\n",
    "                         'select the final cluster id label')\n",
    "parser.add_argument('--topk', type=int, default=5,\n",
    "                    help='if majority_vote is set to True, then retrieval binder selects the topk nearest encodings at '\n",
    "                         'inference to identify the most likely cluster assignment')\n",
    "parser.add_argument('--thresh_attn_obj_slots', type=float, default=0.98,\n",
    "                    help='threshold value for determining the object slots from set of slots, '\n",
    "                         'based on attention weight values (between 0. and 1.)(see retrievalbinder for usage).'\n",
    "                         'This should be reestimated for every dataset individually if thresh_count_obj_slots is '\n",
    "                         'not set to 0.')\n",
    "parser.add_argument('--thresh_count_obj_slots', type=int, default=-1,\n",
    "                    help='threshold value (>= -1) for determining the number of object slots from a set of slots, '\n",
    "                         '-1 indicates we wish to use all slots, i.e. no preselection is made'\n",
    "                         '0 indicates we just take that slot with the maximum slot attention value,'\n",
    "                         '1 indicates we take the maximum count of high attn weights (based on thresh_attn_ob_slots), '\n",
    "                         'otherwise those slots that contain a number of values above thresh_attn_obj_slots are chosen' \n",
    "                         '(see retrievalbinder for usage)')\n",
    "\n",
    "parser.add_argument('--device', default=DEVICE)\n",
    "\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "MODEL_SEED=1\n",
    "BLOCK_ID_POS = 11\n",
    "N_TRAIN_BATCHES=40\n",
    "\n",
    "args.checkpoint_path = f'logs/clevr4_600_epochs/clevr4_sysbind_orig_seed{MODEL_SEED}/best_model.pt'\n",
    "args.retrieval_corpus_path = f'logs/clevr4_600_epochs/clevr4_sysbind_orig_seed{MODEL_SEED}/block_concept_dicts.pkl'\n",
    "args.thresh_count_obj_slots = 0\n",
    "args.num_blocks = 16\n",
    "args.num_categories = 5 \n",
    "args.majority_vote = False\n",
    "args.data_path = '/workspace/datasets-local/CLEVR-4-1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfa5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train the classifier on the original validation set and test on the original test set\n",
    "train_dataset = CLEVR4_1_WithAnnotations_LeftRight(\n",
    "    root=args.data_path, phase=\"val\", img_size=args.image_size, max_num_objs=args.num_slots,\n",
    "    num_categories=args.num_categories, perc_imgs=args.perc_imgs\n",
    ")\n",
    "test_dataset = CLEVR4_1_WithAnnotations_LeftRight(\n",
    "    root=args.data_path, phase=\"test\", img_size=args.image_size, max_num_objs=args.num_slots,\n",
    "    num_categories=args.num_categories, perc_imgs=1.\n",
    ")\n",
    "\n",
    "loader_kwargs = {\n",
    "    \"batch_size\": 20,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": args.num_workers,\n",
    "    \"pin_memory\": True,\n",
    "    \"drop_last\": True,\n",
    "}\n",
    "train_loader = DataLoader(train_dataset, **loader_kwargs)\n",
    "loader_kwargs = {\n",
    "    \"batch_size\": args.batch_size,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": args.num_workers,\n",
    "    \"pin_memory\": True,\n",
    "    \"drop_last\": True,\n",
    "}\n",
    "test_loader = DataLoader(test_dataset, **loader_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a9406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_block_encs_and_pos_labels(loader, retbind_model, n_batches=-1, cont=True):\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    if n_batches == -1:\n",
    "        n_batches = len(loader)\n",
    "    \n",
    "    all_labels = []\n",
    "    all_codes = []\n",
    "    all_imgs = []\n",
    "    for i, sample in enumerate(loader):\n",
    "        \n",
    "        if i == n_batches:\n",
    "            break\n",
    "\n",
    "        img_locs = sample[-1]\n",
    "        sample = sample[:-1]\n",
    "        # imgs, _, annotations, _, class_labels, _ = map(lambda x: x.to(args.device), sample)\n",
    "        imgs, _, annotations, annotations_multihot = map(lambda x: x.to(args.device), sample)\n",
    "\n",
    "        # encode image with whatever model is being used\n",
    "        if cont:\n",
    "            encs = retbind_model.model.encode(imgs)\n",
    "            # make sure only 1 object is selected\n",
    "            assert encs[0].shape[1] == 1\n",
    "            encs_blocked = encs[3][0].squeeze(dim=1)\n",
    "        else:\n",
    "            encs = retbind_model.encode(imgs)[0]\n",
    "            assert encs.shape[1] == 1\n",
    "            encs_blocked = encs.squeeze(dim=1)\n",
    "            \n",
    "        # get position label\n",
    "        pos_label = annotations[:, :, 4].squeeze(dim=1)\n",
    "\n",
    "        all_labels.extend(pos_label.detach().cpu().numpy())\n",
    "        all_codes.extend(encs_blocked.detach().cpu().numpy())\n",
    "        all_imgs.extend(imgs.detach().cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_codes = np.array(all_codes)\n",
    "    all_imgs = np.array(all_imgs)\n",
    "\n",
    "    return all_codes, all_labels, all_imgs\n",
    "\n",
    "\n",
    "def comp_pos_acc_per_dt(train_encs, train_labels, test_encs, test_labels):\n",
    "    clf = DecisionTreeClassifier(random_state=0)\n",
    "    \n",
    "    # fit clf on training encodings and labels\n",
    "    clf.fit(train_encs, train_labels)\n",
    "    # apply to test encodings\n",
    "    test_pred = clf.predict(test_encs)\n",
    "    # compute accuracy per block on test set\n",
    "    return metrics.balanced_accuracy_score(test_labels, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb281fb6",
   "metadata": {},
   "source": [
    "# 1. Identify the relevant position encoding block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813ff8e",
   "metadata": {},
   "source": [
    "### Gather all block-wise codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51245abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading retrieval corpus from logs/clevr4_600_epochs/clevr4_sysbind_orig_seed1/block_concept_dicts.pkl ...\n",
      "loaded ...logs/clevr4_600_epochs/clevr4_sysbind_orig_seed1/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# retbind_model = NeuralConceptBinder(args)           # automatically loads the model internally\n",
    "#                                         # if I want to have \"normal\" model encodings, I should use the SysBinder...\n",
    "# retbind_model.to(DEVICE);\n",
    "# retbind_model.eval();\n",
    "\n",
    "# train_encs, train_labels = gather_block_encs_and_pos_labels(train_loader, retbind_model, n_batches=-1, cont=True)\n",
    "# test_encs, test_labels = gather_block_encs_and_pos_labels(test_loader, retbind_model, n_batches=-1, cont=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b1de0",
   "metadata": {},
   "source": [
    "### Iterate over each block and test if DT can somewhat classify the positions based on the block encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0cbc572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 57.940000000000005\n",
      "1: 75.17\n",
      "2: 56.88999999999999\n",
      "3: 64.71000000000001\n",
      "4: 62.62\n",
      "5: 61.370000000000005\n",
      "6: 53.769999999999996\n",
      "7: 67.89\n",
      "8: 58.01\n",
      "9: 59.37\n",
      "10: 57.24\n",
      "11: 94.75\n",
      "12: 63.13999999999999\n",
      "13: 61.22\n",
      "14: 58.64\n",
      "15: 62.29\n",
      "\n",
      "Relevant block id for position: 11\n"
     ]
    }
   ],
   "source": [
    "# accs_per_block = []\n",
    "# for block_id in range(args.num_blocks):\n",
    "#     acc = comp_pos_acc_per_dt(train_encs[:, block_id], train_labels, test_encs[:, block_id], test_labels)\n",
    "#     accs_per_block.append(acc)\n",
    "\n",
    "# for block_id in range(args.num_blocks):\n",
    "#     print(f'{block_id}: {np.round(100*accs_per_block[block_id], 2)}')\n",
    "\n",
    "# BLOCK_ID_POS = np.argmax(accs_per_block)\n",
    "# print(f'\\nRelevant block id for position: {BLOCK_ID_POS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d46d6",
   "metadata": {},
   "source": [
    "# 2. Compute Acc of unrevised Retrievalbinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7bf29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading retrieval corpus from logs/clevr4_600_epochs/clevr4_sysbind_orig_seed1/block_concept_dicts.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ...logs/clevr4_600_epochs/clevr4_sysbind_orig_seed1/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "args.checkpoint_path = f'logs/clevr4_600_epochs/clevr4_sysbind_orig_seed{MODEL_SEED}/best_model.pt'\n",
    "args.retrieval_corpus_path = f'logs/clevr4_600_epochs/clevr4_sysbind_orig_seed{MODEL_SEED}/block_concept_dicts.pkl'\n",
    "\n",
    "retbind_model = NeuralConceptBinder(args)           # automatically loads the model internally\n",
    "                                        # if I want to have \"normal\" model encodings, I should use the SysBinder...\n",
    "\n",
    "retbind_model.to(DEVICE);\n",
    "retbind_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac00c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encs, train_labels, train_imgs = gather_block_encs_and_pos_labels(train_loader, retbind_model, \n",
    "                                                            n_batches=-1, cont=False)\n",
    "train_encs = train_encs[:, BLOCK_ID_POS]\n",
    "\n",
    "test_encs, test_labels, _ = gather_block_encs_and_pos_labels(test_loader, retbind_model, \n",
    "                                                          n_batches=-1, cont=False)\n",
    "test_encs = test_encs[:, BLOCK_ID_POS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b1f0c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select one example of each cluster\n",
    "# unique_ids = np.unique(train_encs, return_index=True)[1]\n",
    "# train_encs = train_encs[unique_ids]\n",
    "# train_labels = train_labels[unique_ids]\n",
    "# train_imgs = train_imgs[unique_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "989a8cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Acc. of unrevised retrieval binder for left-right position classification: 94.88\n"
     ]
    }
   ],
   "source": [
    "# acc = comp_pos_acc_per_dt(np.expand_dims(train_encs, axis=1), train_labels, \n",
    "#                           np.expand_dims(test_encs, axis=1), test_labels)\n",
    "# print(f'Balanced Acc. of unrevised retrieval binder for left-right position classification: {np.round(100*acc, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a96eeeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Acc. of unrevised retrieval binder for left-right position classification: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "acc = np.round(100 * metrics.balanced_accuracy_score(test_labels, test_encs), 2)\n",
    "print(f'Balanced Acc. of unrevised retrieval binder for left-right position classification: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473aa37",
   "metadata": {},
   "source": [
    "# 3. Merge concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "137e09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "    \n",
    "# fit clf on training encodings and labels\n",
    "clf.fit(np.expand_dims(train_encs, axis=1), train_labels)\n",
    "\n",
    "cls_per_cluster = []\n",
    "for i in range(int(np.max(train_encs))):\n",
    "    cls_per_cluster.append(clf.predict([[int(i)]]))\n",
    "\n",
    "cls_per_cluster = np.array(cls_per_cluster).squeeze(1)\n",
    "cls_per_cluster.shape\n",
    "\n",
    "ids_0 = np.where(cls_per_cluster == 0)[0]\n",
    "ids_1 = np.where(cls_per_cluster == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "450caef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encs_merge = []\n",
    "for test_enc in test_encs:\n",
    "    if test_enc in ids_0:\n",
    "        test_encs_merge.append(0)\n",
    "    elif test_enc in ids_1:\n",
    "        test_encs_merge.append(1)\n",
    "    else:\n",
    "        p = np.random.rand()\n",
    "        if p > 0.5:\n",
    "            test_encs_merge.append(1)\n",
    "        else:\n",
    "            test_encs_merge.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b053dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Acc. of merged retrieval binder for left-right position classification: 95.5\n"
     ]
    }
   ],
   "source": [
    "acc = np.round(100 * metrics.balanced_accuracy_score(test_labels, test_encs_merge), 2)\n",
    "print(f'Balanced Acc. of merged retrieval binder for left-right position classification: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbffd1f0",
   "metadata": {},
   "source": [
    "# 4. Add positional exemplars to corpus at relevant block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9db243dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_concept(block_corpus, delete_id):\n",
    "    \"\"\"\n",
    "    Removes encodings and corresponding information entirely from the cluster identified via 'delete_id'\n",
    "    \"\"\"\n",
    "    # i.e. 'prototypes', 'exemplars', 'sivm_basis'\n",
    "    representation_keys = list(block_corpus.keys())\n",
    "    representation_keys.remove('ids')\n",
    "\n",
    "    # identify which encodings to keep and which not to keep\n",
    "    del_ids = np.where(block_corpus['ids'].detach().cpu().numpy() == delete_id)[0]\n",
    "    keep_ids = np.where(block_corpus['ids'].detach().cpu().numpy() != delete_id)[0]\n",
    "    # number of individual clusters altogether\n",
    "    n_clusters = len(np.unique(block_corpus['ids'].detach().cpu().numpy()))\n",
    "\n",
    "    block_corpus['types'] = [ele for idx, ele in enumerate(block_corpus['types']) if idx in keep_ids]\n",
    "    block_corpus['encs'] = block_corpus['encs'][keep_ids]\n",
    "    # finally remove the ids themselves, i.e. keep only relevant ones\n",
    "    block_corpus['ids'] = block_corpus['ids'][keep_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98740982",
   "metadata": {},
   "source": [
    "### First delete all encodings from block corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07f981ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading retrieval corpus from logs/clevr4_600_epochs/clevr4_sysbind_orig_seed1/block_concept_dicts.pkl ...\n",
      "loaded ...logs/clevr4_600_epochs/clevr4_sysbind_orig_seed1/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "args.checkpoint_path = f'logs/clevr4_600_epochs/clevr4_sysbind_orig_seed{MODEL_SEED}/best_model.pt'\n",
    "args.retrieval_corpus_path = f'logs/clevr4_600_epochs/clevr4_sysbind_orig_seed{MODEL_SEED}/block_concept_dicts.pkl'\n",
    "\n",
    "retbind_model_revise = NeuralConceptBinder(args)           # automatically loads the model internally\n",
    "                                        # if I want to have \"normal\" model encodings, I should use the SysBinder...\n",
    "\n",
    "retbind_model_revise.to(DEVICE);\n",
    "retbind_model_revise.eval();\n",
    "\n",
    "concept_ids = np.unique(retbind_model_revise.retrieval_corpus[BLOCK_ID_POS]['ids'].detach().cpu().numpy())\n",
    "for delete_concept_id in concept_ids:\n",
    "    remove_concept(\n",
    "        retbind_model_revise.retrieval_corpus[BLOCK_ID_POS],\n",
    "        delete_id=delete_concept_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddf05d",
   "metadata": {},
   "source": [
    "### Now extract the block_encodings of users exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd55c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_right_5 = [\n",
    "    'CLEVR_4_classid_0_000000.png',\n",
    "    'CLEVR_4_classid_0_000003.png',\n",
    "    'CLEVR_4_classid_0_000007.png',\n",
    "    'CLEVR_4_classid_0_000017.png',\n",
    "    'CLEVR_4_classid_0_000028.png',\n",
    "]\n",
    "\n",
    "fns_left_5 = [\n",
    "    'CLEVR_4_classid_0_000004.png',\n",
    "    'CLEVR_4_classid_0_000010.png',\n",
    "    'CLEVR_4_classid_0_000018.png',\n",
    "    'CLEVR_4_classid_0_000031.png',\n",
    "    'CLEVR_4_classid_0_000041.png',\n",
    "]\n",
    "\n",
    "fns_right_20 = [\n",
    "    'CLEVR_4_classid_0_000000.png',\n",
    "    'CLEVR_4_classid_0_000003.png',\n",
    "    'CLEVR_4_classid_0_000007.png',\n",
    "    'CLEVR_4_classid_0_000017.png',\n",
    "    'CLEVR_4_classid_0_000028.png',\n",
    "    'CLEVR_4_classid_0_000073.png',\n",
    "    'CLEVR_4_classid_0_000085.png',\n",
    "    'CLEVR_4_classid_0_000088.png',\n",
    "    'CLEVR_4_classid_0_000090.png',\n",
    "    'CLEVR_4_classid_0_000109.png',\n",
    "    'CLEVR_4_classid_0_000128.png',\n",
    "    'CLEVR_4_classid_0_000021.png',\n",
    "    'CLEVR_4_classid_0_000029.png',\n",
    "    'CLEVR_4_classid_0_000046.png',\n",
    "    'CLEVR_4_classid_0_000052.png',\n",
    "    'CLEVR_4_classid_0_000056.png',\n",
    "    'CLEVR_4_classid_0_000065.png',\n",
    "    'CLEVR_4_classid_0_000078.png',\n",
    "    'CLEVR_4_classid_0_000079.png',\n",
    "    'CLEVR_4_classid_0_000083.png',\n",
    "]\n",
    "\n",
    "fns_left_20 = [\n",
    "    'CLEVR_4_classid_0_000004.png',\n",
    "    'CLEVR_4_classid_0_000010.png',\n",
    "    'CLEVR_4_classid_0_000018.png',\n",
    "    'CLEVR_4_classid_0_000031.png',\n",
    "    'CLEVR_4_classid_0_000041.png',\n",
    "    'CLEVR_4_classid_0_000053.png',\n",
    "    'CLEVR_4_classid_0_000055.png',\n",
    "    'CLEVR_4_classid_0_000064.png',\n",
    "    'CLEVR_4_classid_0_000089.png',\n",
    "    'CLEVR_4_classid_0_000094.png',\n",
    "    'CLEVR_4_classid_0_000103.png',\n",
    "    'CLEVR_4_classid_0_000041.png',\n",
    "    'CLEVR_4_classid_0_000044.png',\n",
    "    'CLEVR_4_classid_0_000049.png',\n",
    "    'CLEVR_4_classid_0_000058.png',\n",
    "    'CLEVR_4_classid_0_000059.png',\n",
    "    'CLEVR_4_classid_0_000116.png',\n",
    "    'CLEVR_4_classid_0_000160.png',\n",
    "    'CLEVR_4_classid_0_000149.png',\n",
    "    'CLEVR_4_classid_0_000240.png',\n",
    "]\n",
    "\n",
    "\n",
    "revision_sample_pths_right_5 = []\n",
    "for sample_fn in fns_right_5:\n",
    "    revision_sample_pths_right_5.append(f'{args.data_path}train/images/{sample_fn}')\n",
    "    \n",
    "revision_sample_pths_left_5 = []\n",
    "for sample_fn in fns_left_5:\n",
    "    revision_sample_pths_left_5.append(f'{args.data_path}train/images/{sample_fn}')\n",
    "\n",
    "revision_sample_pths_right_20 = []\n",
    "for sample_fn in fns_right_20:\n",
    "    revision_sample_pths_right_20.append(f'{args.data_path}train/images/{sample_fn}')\n",
    "    \n",
    "revision_sample_pths_left_20 = []\n",
    "for sample_fn in fns_left_20:\n",
    "    revision_sample_pths_left_20.append(f'{args.data_path}train/images/{sample_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dc564ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_pths, img_size):\n",
    "        self.img_pths = img_pths\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_pths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_pths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize((self.img_size, self.img_size))\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# dataset_right_5 = CustomImageDataset(revision_sample_pths_right_5, args.image_size)\n",
    "# dataset_left_5 = CustomImageDataset(revision_sample_pths_left_5, args.image_size)\n",
    "\n",
    "dataset_right_20 = CustomImageDataset(revision_sample_pths_right_20, args.image_size)\n",
    "dataset_left_20 = CustomImageDataset(revision_sample_pths_left_20, args.image_size)\n",
    "\n",
    "loader_kwargs = {\n",
    "    \"batch_size\": 1,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": args.num_workers,\n",
    "    \"pin_memory\": True,\n",
    "    \"drop_last\": True,\n",
    "}\n",
    "\n",
    "# revision_loader_right_5 = DataLoader(dataset_right_5, **loader_kwargs)\n",
    "# revision_loader_left_5 = DataLoader(dataset_left_5, **loader_kwargs)\n",
    "\n",
    "revision_loader_right_20 = DataLoader(dataset_right_20, **loader_kwargs)\n",
    "revision_loader_left_20 = DataLoader(dataset_left_20, **loader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c649bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset_and_add_to_corpus(model, loader, rel_block_id, new_id, args):\n",
    "    # iterate over revision samples and encode them\n",
    "    for i, imgs in enumerate(loader):\n",
    "        imgs = imgs.to(args.device)\n",
    "\n",
    "        # encode image \n",
    "        encs = model.model.encode(imgs)\n",
    "        encs_blocked = encs[3][0][:,:,rel_block_id].squeeze(dim=0)\n",
    "        # add encoding and corresponding information to retrieval corpus\n",
    "        model.retrieval_corpus[rel_block_id]['types'].append('exemplar')\n",
    "        model.retrieval_corpus[rel_block_id]['encs'] = torch.cat(\n",
    "            (model.retrieval_corpus[rel_block_id]['encs'], encs_blocked), 0\n",
    "        )\n",
    "        model.retrieval_corpus[rel_block_id]['ids'] = torch.cat(\n",
    "            (model.retrieval_corpus[rel_block_id]['ids'], torch.tensor([new_id], device=args.device)), 0\n",
    "        )\n",
    "\n",
    "# encode_dataset_and_add_to_corpus(retbind_model_revise, revision_loader_right_5, BLOCK_ID_POS, 1, args)\n",
    "# encode_dataset_and_add_to_corpus(retbind_model_revise, revision_loader_left_5, BLOCK_ID_POS, 0, args)\n",
    "        \n",
    "encode_dataset_and_add_to_corpus(retbind_model_revise, revision_loader_right_20, BLOCK_ID_POS, 1, args)\n",
    "encode_dataset_and_add_to_corpus(retbind_model_revise, revision_loader_left_20, BLOCK_ID_POS, 0, args)\n",
    "# retbind_model_revise.retrieval_corpus[BLOCK_ID_POS];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3975dde",
   "metadata": {},
   "source": [
    "# 4. Compute Acc of revised Retrievalbinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40496b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encs_revise, test_labels_revise, _ = gather_block_encs_and_pos_labels(test_loader, retbind_model_revise, \n",
    "                                                                        n_batches=-1, cont=False)\n",
    "test_encs_revise = test_encs_revise[:, BLOCK_ID_POS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0c8fe82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each sample used ro ret_binder compute new encoding\n",
    "# train_encs_revise = []\n",
    "# for img in train_imgs:\n",
    "#     encs = retbind_model_revise.encode(torch.tensor(img).unsqueeze(dim=0))[0]\n",
    "#     train_encs_revise.append(encs.squeeze(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# train_encs_revise = np.array(train_encs_revise).squeeze(axis=1)\n",
    "# train_labels_revise = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b9f3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_revise = comp_pos_acc_per_dt(np.expand_dims(train_encs_revise[:, BLOCK_ID_POS], axis=1), train_labels_revise, \n",
    "#                           np.expand_dims(test_encs_revise[:, BLOCK_ID_POS], axis=1), test_labels_revise)\n",
    "# print(f'Balanced Acc. of revised retrieval binder for left-right position classification: {np.round(100*acc_revise, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027e9bd",
   "metadata": {},
   "source": [
    "### 20 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2c0b412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Acc. of revised retrieval binder for left-right position classification: 93.97\n"
     ]
    }
   ],
   "source": [
    "acc = np.round(100 * metrics.balanced_accuracy_score(test_labels_revise, test_encs_revise), 2)\n",
    "print(f'Balanced Acc. of revised retrieval binder for left-right position classification: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ca726",
   "metadata": {},
   "source": [
    "### 5 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7402303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Acc. of revised retrieval binder for left-right position classification: 88.71\n"
     ]
    }
   ],
   "source": [
    "acc = np.round(100 * metrics.balanced_accuracy_score(test_labels_revise, test_encs_revise), 2)\n",
    "print(f'Balanced Acc. of revised retrieval binder for left-right position classification: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2be0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
